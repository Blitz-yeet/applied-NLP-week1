{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a05e673",
   "metadata": {},
   "source": [
    "\n",
    "# 1) Frequent Words = Literary Fingerprints\n",
    "\n",
    "This notebook compares **word frequency** between our two toy texts:\n",
    "- *Pet Semetary* (here referenced as **pet**)\n",
    "- *The Shining* (here referenced as **Shining**)\n",
    "\n",
    "We practice simple tokenization and frequency analysis, then discuss\n",
    "what's **meaningful signal** vs. **noise** in the results, and how to\n",
    "improve the method (normalization, keyness, etc).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a2f5446",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "from collections import Counter\n",
    "import math\n",
    "# at least one letter, apostrophes only allowed inside (keeps \"don't\", drops \"'\" alone)\n",
    "WORD_RE = re.compile(r\"[A-Za-z]+(?:'[A-Za-z]+)*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c5da6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_texts(local_pet: str = '../data/PetSemetary.txt',\n",
    "               local_shining: str = '../data/TheShining.txt'):\n",
    "    p1, p2 = Path(local_pet), Path(local_shining)\n",
    "\n",
    "    if not p1.exists():\n",
    "        raise FileNotFoundError(f\"Missing file: {p1}\")\n",
    "    if not p2.exists():\n",
    "        raise FileNotFoundError(f\"Missing file: {p2}\")\n",
    "\n",
    "    pet    = p1.read_text(encoding='utf-8', errors='ignore')\n",
    "    shining = p2.read_text(encoding='utf-8', errors='ignore')\n",
    "    return pet, shining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b64e2892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text: str) -> str:\n",
    "    if not text:\n",
    "        return ''\n",
    "    # normalize curly quotes to ASCII '\n",
    "    text = text.replace(\"’\", \"'\").replace(\"‘\", \"'\")\n",
    "    # normalize Windows endings\n",
    "    text = text.replace('\\r\\n', '\\n')\n",
    "    # join hyphenated line breaks\n",
    "    text = re.sub(r\"-\\s*\\n\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def words(text: str):\n",
    "    return WORD_RE.findall(text.lower())\n",
    "\n",
    "def sentences(text: str):\n",
    "    return [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618c7d96",
   "metadata": {},
   "source": [
    "\n",
    "## Load & Normalize\n",
    "We load both texts using **inline path checks** and then apply a simple normalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcbfb253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pet Sematary chars: 812,353 | The Shining chars: 905,869\n",
      "Pet Sematary words (raw): 147,144 | The Shining words (raw): 162,085\n",
      "Pet Sematary sentences: 9,269 | The Shining sentences: 12,914\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Lightweight stopworded top-words helper ===\n",
    "\n",
    "def top_words(words_list, min_len=4, extra_stop=None, n=30):\n",
    "    \"\"\"Return top-N frequent words after lightweight filtering.\"\"\"\n",
    "    base_stop = {\n",
    "        'the','and','to','of','a','i','it','in','that','was','he','you','is','for','on','as',\n",
    "        'with','his','her','at','be','she','had','not','but','said','they','them','this','so','all','one','very',\n",
    "        'there','what','were','from','have','would','could','when','been','their','we','my','me','or','by','up','no','out','if',\n",
    "        'pet'   # book-specific: remove if you want\n",
    "    }\n",
    "    if extra_stop:\n",
    "        base_stop |= set(extra_stop)\n",
    "\n",
    "    c = Counter(w for w in words_list if len(w) >= min_len and w not in base_stop)\n",
    "    return c.most_common(n)\n",
    "\n",
    "\n",
    "# === Load & Normalize ===\n",
    "\n",
    "# Load raw texts\n",
    "pet_raw, shining_raw = load_texts()\n",
    "\n",
    "# Normalize\n",
    "pet_norm     = normalize(pet_raw)\n",
    "shining_norm = normalize(shining_raw)\n",
    "\n",
    "print(f\"Pet Sematary chars: {len(pet_norm):,} | The Shining chars: {len(shining_norm):,}\")\n",
    "\n",
    "# Tokenize (RAW tokens before cleaning)\n",
    "pet_tokens_raw     = words(pet_norm)\n",
    "shining_tokens_raw = words(shining_norm)\n",
    "\n",
    "pet_sentences     = sentences(pet_norm)\n",
    "shining_sentences = sentences(shining_norm)\n",
    "\n",
    "print(f\"Pet Sematary words (raw): {len(pet_tokens_raw):,} | The Shining words (raw): {len(shining_tokens_raw):,}\")\n",
    "print(f\"Pet Sematary sentences: {len(pet_sentences):,} | The Shining sentences: {len(shining_sentences):,}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b194521",
   "metadata": {},
   "source": [
    "\n",
    "## Tokenize\n",
    "We use a simple regex tokenizer (letters + apostrophes). For more serious work,\n",
    "consider spaCy or stanza for tagging and lemmatization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71a843aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pet Sematary words (raw): 147,144 | The Shining words (raw): 162,085\n",
      "Pet Sematary sentences: 9,269 | The Shining sentences: 12,914\n"
     ]
    }
   ],
   "source": [
    "# === Tokenize ===\n",
    "\n",
    "# THESE should be strings, so we load and normalize first\n",
    "pet_raw, shining_raw = load_texts()\n",
    "pet_norm     = normalize(pet_raw)\n",
    "shining_norm = normalize(shining_raw)\n",
    "\n",
    "# Now tokenize TEXT STRINGS (not lists!)\n",
    "pet_words_raw     = WORD_RE.findall(pet_norm.lower())\n",
    "shining_words_raw = WORD_RE.findall(shining_norm.lower())\n",
    "\n",
    "pet_sentences     = sentences(pet_norm)\n",
    "shining_sentences = sentences(shining_norm)\n",
    "\n",
    "print(f\"Pet Sematary words (raw): {len(pet_words_raw):,} | The Shining words (raw): {len(shining_words_raw):,}\")\n",
    "print(f\"Pet Sematary sentences: {len(pet_sentences):,} | The Shining sentences: {len(shining_sentences):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188dd619",
   "metadata": {},
   "source": [
    "\n",
    "## Top Words (after basic stopwords)\n",
    "The list is **partly signal, partly noise**—use it to start discussion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec91f70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pet Sematary words (clean): 142,499 | The Shining words (clean): 156,970\n",
      "Sample Pet tokens: ['pet', 'sematary', 'by', 'stephen', 'king', 'published', 'jjjjj', 'iiiii', 'table', 'of', 'contents', 'dedication', 'introduction', 'part', 'the', 'pet', 'sematary', 'chapter', 'thru', 'chapter']\n",
      "Sample Shining tokens: ['the', 'shining', 'by', 'stephen', 'king', 'this', 'is', 'for', 'joe', 'hill', 'king', 'who', 'shines', 'on', 'my', 'editor', 'on', 'this', 'book', 'as']\n"
     ]
    }
   ],
   "source": [
    "JUNK = {\n",
    "    \"ll\",\"s\",\"t\",\"ve\",\"re\",\"m\",\"d\",\n",
    "    \"didn\",\"don\",\"doesn\",\"isn\",\"wasn\",\"aren\",\"weren\",\"ain\",\n",
    "    \"couldn\",\"wouldn\",\"shouldn\",\"hadn\",\"haven\",\"hasn\",\"mustn\"\n",
    "}\n",
    "\n",
    "def clean_tokens(tokens):\n",
    "    cleaned = []\n",
    "    for w in tokens:\n",
    "        if w in JUNK:\n",
    "            continue\n",
    "        if len(w) == 1:    # removes 'i', 'a', single letters etc.\n",
    "            continue\n",
    "        cleaned.append(w)\n",
    "    return cleaned\n",
    "\n",
    "pet_words     = clean_tokens(pet_tokens_raw)\n",
    "shining_words = clean_tokens(shining_tokens_raw)\n",
    "\n",
    "print(f\"Pet Sematary words (clean): {len(pet_words):,} | The Shining words (clean): {len(shining_words):,}\")\n",
    "print(\"Sample Pet tokens:\", pet_words[:20])\n",
    "print(\"Sample Shining tokens:\", shining_words[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10829309",
   "metadata": {},
   "source": [
    "\n",
    "## Discussion\n",
    "- Which frequent words are **thematically meaningful** vs. artifacts of stopwording?\n",
    "- Do **chess terms** (e.g., *queen*, *white*, *red*) show higher distinctiveness in *Looking-Glass*?\n",
    "- Do **spatial/falling terms** (e.g., *down*, *rabbit*) show higher distinctiveness in *Wonderland*?\n",
    "- How would **lemmatization** (e.g., *think/thinks/thought*) change results?\n",
    "- Implement **per_10k(count,total_words)** and **lolookingglass_likelihood(k1,n1,k2,n2) (Dunning’s G²)**, then list the 20 most distinctive words between Wonderland and Looking-Glass with per-10k rates and briefly argue which are meaningful vs. artifacts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36d4fbd",
   "metadata": {},
   "source": [
    "## Optional continution:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2825a8f8",
   "metadata": {},
   "source": [
    "\n",
    "## Distinctiveness via Log-Likelihood (Keyness)\n",
    "Raw frequency is not enough. Compute **G²** to find words that are *distinctive* of each book.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "285caabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_10k(count: int, total_words: int) -> float:\n",
    "    \"\"\"Normalize a raw count per 10,000 words for fair comparisons.\"\"\"\n",
    "    return (count / max(1, total_words)) * 10000.0\n",
    "\n",
    "\n",
    "def log_likelihood(k1: int, n1: int, k2: int, n2: int) -> float:\n",
    "    \"\"\" log-likelihood (G^2) keyness score for word distinctiveness.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    k1 : int  Frequency in corpus A\n",
    "    n1 : int  Total words in corpus A\n",
    "    k2 : int  Frequency in corpus B\n",
    "    n2 : int  Total words in corpus B\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        G^2 value; larger absolute values indicate stronger distinctiveness.\n",
    "        Direction should be interpreted by comparing rates (per_10k) or counts.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Symmetric measure widely used for corpus comparison.\n",
    "    - Great classroom upgrade over raw frequency lists.\n",
    "    \"\"\"\n",
    "    E1 = n1 * (k1 + k2) / max(1, (n1 + n2))\n",
    "    E2 = n2 * (k1 + k2) / max(1, (n1 + n2))\n",
    "\n",
    "    def term(k, E):\n",
    "        return 0.0 if k == 0 or E == 0 else k * math.log(k / E)\n",
    "\n",
    "    return 2.0 * (term(k1, E1) + term(k2, E2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d91d5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most distinctive (either direction):\n",
      "       louis  G2= 2398.8  W:113.33/10k  LG:  0.00/10k\n",
      "       danny  G2= 1165.3  W:  0.00/10k  LG: 57.46/10k\n",
      "        jack  G2=  918.6  W:  0.28/10k  LG: 47.46/10k\n",
      "      rachel  G2=  760.6  W: 36.56/10k  LG:  0.06/10k\n",
      "         jud  G2=  751.6  W: 35.51/10k  LG:  0.00/10k\n",
      "       ellie  G2=  604.5  W: 28.56/10k  LG:  0.00/10k\n",
      "        gage  G2=  582.3  W: 27.51/10k  LG:  0.00/10k\n",
      "       wendy  G2=  463.8  W:  0.00/10k  LG: 22.87/10k\n",
      "   hallorann  G2=  436.7  W:  0.00/10k  LG: 21.53/10k\n",
      "      church  G2=  307.8  W: 16.84/10k  LG:  0.38/10k\n",
      "    overlook  G2=  236.4  W:  0.00/10k  LG: 11.66/10k\n",
      "      ullman  G2=  231.3  W:  0.00/10k  LG: 11.40/10k\n",
      "       hotel  G2=  217.7  W:  0.21/10k  LG: 12.04/10k\n",
      "      gage's  G2=  173.8  W:  8.21/10k  LG:  0.00/10k\n",
      "     danny's  G2=  173.1  W:  0.00/10k  LG:  8.54/10k\n",
      "         cat  G2=  172.1  W: 10.88/10k  LG:  0.57/10k\n",
      "        tony  G2=  160.2  W:  0.07/10k  LG:  8.41/10k\n",
      "        snow  G2=  157.7  W:  0.77/10k  LG: 11.08/10k\n",
      "       steve  G2=  154.7  W:  7.79/10k  LG:  0.06/10k\n",
      "    torrance  G2=  153.7  W:  0.00/10k  LG:  7.58/10k\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build frequency dictionaries\n",
    "cw = Counter(pet_words)\n",
    "cg = Counter(shining_words)\n",
    "nW, nG = sum(cw.values()), sum(cg.values())\n",
    "\n",
    "# Compare a candidate set (union of top ~500 from each to keep it fast)\n",
    "candidates = set([w for w,_ in cw.most_common(500)] + [w for w,_ in cg.most_common(500)])\n",
    "\n",
    "rows = []\n",
    "for w in candidates:\n",
    "    g2 = log_likelihood(cw[w], nW, cg[w], nG)\n",
    "    rows.append((g2, w, per_10k(cw[w], nW), per_10k(cg[w], nG)))\n",
    "\n",
    "# Sort by distinctiveness (descending)\n",
    "rows.sort(reverse=True)\n",
    "\n",
    "print(\"Most distinctive (either direction):\")\n",
    "for g2, w, a10, b10 in rows[:20]:\n",
    "    print(f\"{w:>12}  G2={g2:7.1f}  W:{a10:6.2f}/10k  LG:{b10:6.2f}/10k\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a16670a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
